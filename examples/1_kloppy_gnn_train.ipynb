{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ€ unravel kloppy into graph neural network using the _new_ Polars back-end!\n",
    "\n",
    "First run `pip install unravelsports` if you haven't already!\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unravelsports --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this in-depth walkthrough we'll discuss everything the `unravelsports` package has to offer for converting a [Kloppy](https://github.com/PySport/kloppy) dataset of soccer tracking data into graphs for training binary classification graph neural networks using the [Spektral](https://graphneural.network/) library, and a newly added (version==0.3.0+) [Polars](https://pola.rs/) back-end.\n",
    "\n",
    "This walkthrough will touch on a lot of the concepts from [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn). It is strongly advised to first read the [research paper (pdf)](https://ussf-ssac-23-soccer-gnn.s3.us-east-2.amazonaws.com/public/Sahasrabudhe_Bekkers_SSAC23.pdf). Some concepts are also explained in the [Graphs FAQ](graphs_faq.md).\n",
    "\n",
    "Step by step we'll show how this package can be used to load soccer positional (tracking) data with `kloppy`, how to convert this data into a `KloppyPolarsDataset`, convert it into \"graphs\", train a Graph Neural Network with `spektral`, evaluate it's performance, save and load the model and finally apply the model to unseen data to make predictions.\n",
    "\n",
    "The powerful Kloppy package allows us to load and standardize data from many providers: Metrica, Sportec, Tracab, SecondSpectrum, StatsPerform and SkillCorner. In this guide we'll use some matches from the [Public SkillCorner Dataset](https://github.com/SkillCorner/opendata).\n",
    "\n",
    "<br>\n",
    "<i>Before we get started it is important to note that the <b>unravelsports</b> library does not have built in functionality to create binary labels, these will need to be supplied by the reader. In this example we use the <b>dummy_labels()</b> functionality that comes with the package. This function creates a single binary label for each frame by randomly assigning it a 0 or 1 value.\n",
    "</i>\n",
    "<br>\n",
    "\n",
    "##### **Contents**\n",
    "\n",
    "- [**1. Imports**](#1-imports).\n",
    "- [**2. Public SkillCorner Data**](#2-public-skillcorner-data).\n",
    "- [**3. â­ _KloppyPolarsDataset_ and _SoccerGraphConverterPolars_**](#2-open-skillcorner-data).\n",
    "- [**4. Load Kloppy Data, Convert & Store**](#4-load-kloppy-data-convert-and-store).\n",
    "- [**5. Creating a Custom Graph Dataset**](#5-creating-a-custom-graph-dataset).\n",
    "- [**6. Prepare for Training**](#6-prepare-for-training).\n",
    "    - [6.1 Split Dataset](#61-split-dataset)\n",
    "    - [6.2 Model Configurations](#62-model-configurations)\n",
    "    - [6.3 Build GNN Model](#63-build-gnn-model)\n",
    "    - [6.4 Create DataLoaders](#64-create-dataloaders)\n",
    "- [**7. GNN Training + Prediction**](#7-training-and-prediction).\n",
    "    - [7.1 Compile Model](#71-compile-model)\n",
    "    - [7.2 Fit Model](#72-fit-model)\n",
    "    - [7.3 Save & Load Model](#73-save--load-model)\n",
    "    - [7.4 Evaluate Model](#74-evaluate-model)\n",
    "    - [7.5 Predict on New Data](#75-predict-on-new-data)\n",
    "\n",
    "â„¹ï¸ [**Graphs FAQ**](graphs_faq.md)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "\n",
    "We import `SoccerGraphConverterPolars` to help us convert from Kloppy positional tracking frames to graphs.\n",
    "\n",
    "With the power of **Kloppy** we can also load data from many providers by importing `metrica`, `sportec`, `tracab`, `secondspectrum`, or `statsperform` from `kloppy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unravel.soccer import SoccerGraphConverterPolars, KloppyPolarsDataset\n",
    "\n",
    "from kloppy import skillcorner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Public SkillCorner Data\n",
    "\n",
    "The `SoccerGraphConverter` class allows processing data from every tracking data provider supported by [PySports Kloppy](https://github.com/PySport/kloppy), namely:\n",
    "- Sportec\n",
    "- Tracab\n",
    "- SecondSpectrum\n",
    "- SkillCorner\n",
    "- StatsPerform\n",
    "- Metrica\n",
    "\n",
    "In this example we're going to use a sample of tracking data from 4 matches of [publicly available SkillCorner data](https://github.com/SkillCorner/opendata). \n",
    "\n",
    "All we need to know for now is that this data is from the following matches:\n",
    "\n",
    "|  id | date_time           | home_team   | away_team   |\n",
    "|---:|:---------------------:|:-----------------------|:-----------------------|\n",
    "|  4039 | 2020-07-02T19:15:00Z | Manchester City        | Liverpool              |\n",
    "|  3749 | 2020-05-26T16:30:00Z | Dortmund               | Bayern Munchen         |\n",
    "|  3518 | 2020-03-08T19:45:00Z | Juventus               | Inter                  |\n",
    "|  3442 | 2020-03-01T20:00:00Z | Real Madrid            | FC Barcelona           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. â­ _KloppyPolarsDataset_ and _SoccerGraphConverterPolars_\n",
    "\n",
    "â„¹ï¸ For more information on:\n",
    "- What a Graph is, check out [Graph FAQ Section A](graphs_faq.ipynb)\n",
    "- What parameters we can pass to the `SoccerGraphConverterPolars`, check out [Graph FAQ Section B](graphs_faq.ipynb)\n",
    "- What features each Graph has, check out [Graph FAQ Section C](graphs_faq.ipynb)\n",
    "\n",
    "------\n",
    "\n",
    "To get started we need to load our tracking data using Kloppy, and subsequently pass this to the `KloppyPolarsDataset`. This `KloppyPolarsDataset` also takes the `ball_carrier_threshold` parameter.\n",
    "\n",
    "ğŸ—’ï¸ KloppyPolarsDataset sets the orientation to `Orientation.BALL_OWNING_TEAM` (ball owning team plays left to right). Except when we don't know who the ball owning team is. This can happen when a data provider does not provide the ball owning team information.\n",
    "If our dataset does not have the ball owning team we infer the ball owning team automatically using the `ball_carrier_threshold` and subsequently change the orientation automatically to be left to right for the ball owning team too.\n",
    "In `SoccerGraphConverter` [deprecated] if the ball owning team was not available we set the orientation to STATIC_HOME_AWAY meaning attacking could happen in two directions. \n",
    "\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 5px; padding: 10px; background-color: ##282C34;\">\n",
    "<pre>\n",
    "kloppy_dataset = skillcorner.load_open_data(\n",
    "    match_id=match_id,\n",
    "    coordinates=\"secondspectrum\",\n",
    "    include_empty_frames=False,\n",
    "    limit=500,  \n",
    ")\n",
    "kloppy_polars_dataset = KloppyPolarsDataset(\n",
    "    kloppy_dataset=kloppy_dataset,\n",
    "    ball_carrier_threshold=25.0\n",
    ")\n",
    "kloppy_polars_dataset.load()\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "#### Graph Identifier(s):\n",
    "After loading the `kloppy_polars_dataset` we now add graph identifiers. We can do this by passing a list of column names on which we want to split our data.\n",
    "\n",
    "ğŸ—’ï¸ When training a model on tracking data it's highly recommended to split data into test/train(/validation) sets by match or period such that all data end up in the same test, train or validation set. This should be done to avoid leaking information between test, train and validation sets. Correctly splitting the final dataset in train, test and validiation sets using these Graph Identifiers is incorporated into `CustomSpektralDataset` (see [Section 6.1](#61-split-dataset) for more information).\n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 5px; padding: 10px; background-color: ##282C34;\">\n",
    "<pre>\n",
    "kloppy_polars_dataset.add_graph_ids(by=[\"game_id\", \"period_id\"])\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "#### Graph Labels\n",
    "\n",
    "Now, we can add our (binary) labels to the dataset. In all examples we do this using `kloppy_polars_dataset.add_dummy_labels()`, but these are random labels and will not help with training.\n",
    "\n",
    "To add useful labels for your task you need to \"join\" a Polars dataframe that contains a column with the required labels to the `kloppy_polars_dataset.data` Polars dataframe. Please note that in this dataframe each row is a single player (or ball) object, and thus each `frame_id` has 23 rows (if all players and ball are observed). All these rows (for a single frame_id) need to have _the same_ label. If your label column is not named `\"label\"` you need to pass the `label_col` (str) parameter to `SoccerGraphConverterPolars`.\n",
    "\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 5px; padding: 10px; background-color: ##282C34;\">\n",
    "<pre>\n",
    "kloppy_polars_dataset.data = (\n",
    "    kloppy_polars_dataset.data\n",
    "    .join(\n",
    "        some_label_dataframe.select([\"game_id\", \"period_id\", \"frame_id\", \"label\"]), \n",
    "        on=[\"game_id\", \"period_id\", \"frame_id\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "### SoccerGraphConverterPolars\n",
    "\n",
    "To get started with the `SoccerGraphConverterPolars` we need to pass one _required_ parameter:\n",
    "- `dataset` (of type `KloppyPolarsDataset`) \n",
    "\n",
    "For a full list of other parameters we can pass to the `SoccerGraphConverterPolars`, check out [Graph FAQ Section B](graphs_faq.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Load Kloppy Data, Convert and Store\n",
    "\n",
    "As mentioned in [Section 2](#2-public-skillcorner-data) we will use 4 matches of SkillCorner data. In the below example we will load the first 500 frames of data from each of these 4 games (we set `limit=500`) to create a dataset of 2,000 samples (Note: We're going to actually have less than 2,000 samples because setting `include_empty_frames=False` means we'll skip some frames in our conversion step).\n",
    "\n",
    "Important things to note:\n",
    "- The `SoccerGraphConverterPolars` handles all necessary steps (like setting the correct coordinate system, and left-right normalization).\n",
    "- We will end up with fewer than 2,000 eventhough we set `limit=500` frames because we set `include_empty_frames=False` and all frames without ball coordinates are automatically ommited.\n",
    "- When using other providers always set `include_empty_frames=False` or `only_alive=True`.\n",
    "- We store the data as individual compressed pickle files, one file for per match. The data that gets stored in the pickle is a list of dictionaries, one dictionary per frame. Each dictionary has keys for the adjacency matrix, node features, edge features, label and graph id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "match_ids = [4039, 3749, 3518, 3442]\n",
    "pickle_folder = \"pickles\"\n",
    "compressed_pickle_file_path = \"{pickle_folder}/{match_id}.pickle.gz\"\n",
    "\n",
    "for match_id in match_ids:\n",
    "    match_pickle_file_path = compressed_pickle_file_path.format(\n",
    "        pickle_folder=pickle_folder, match_id=match_id\n",
    "    )\n",
    "    # if the output file already exists, skip this whole step\n",
    "    if not exists(match_pickle_file_path):\n",
    "        # Load Kloppy dataset\n",
    "        kloppy_dataset = skillcorner.load_open_data(\n",
    "            match_id=match_id,\n",
    "            coordinates=\"secondspectrum\",\n",
    "            include_empty_frames=False,\n",
    "            limit=500,\n",
    "        )\n",
    "        dataset = KloppyPolarsDataset(\n",
    "            kloppy_dataset=kloppy_dataset, ball_carrier_threshold=25.0\n",
    "        )\n",
    "\n",
    "        dataset.add_graph_ids()\n",
    "\n",
    "        dataset.add_dummy_labels()\n",
    "\n",
    "        # Initialize the Graph Converter, with dataset, labels and settings\n",
    "        converter = SoccerGraphConverterPolars(\n",
    "            dataset=dataset,\n",
    "            # Settings\n",
    "            max_player_speed=12.0,\n",
    "            max_ball_speed=28.0,\n",
    "            self_loop_ball=True,\n",
    "            adjacency_matrix_connect_type=\"ball\",\n",
    "            adjacency_matrix_type=\"split_by_team\",\n",
    "            label_type=\"binary\",\n",
    "            defending_team_node_value=0.1,\n",
    "            non_potential_receiver_node_value=0.1,\n",
    "            random_seed=False,\n",
    "            pad=False,\n",
    "            verbose=False,\n",
    "        )\n",
    "        # Compute the graphs and directly store them as a pickle file\n",
    "        converter.to_pickle(file_path=match_pickle_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating a Custom Graph Dataset\n",
    "\n",
    "To easily train our model with the Spektral library we need to use a Spektral dataset object. The `CustomSpektralDataset` class helps us create such an object really easily.\n",
    "\n",
    "- `CustomSpektralDataset` is a [`spektral.data.Dataset`](https://graphneural.network/creating-dataset/). \n",
    "This type of dataset makes it very easy to properly load, train and predict with a Spektral GNN.\n",
    "- The `CustomSpektralDataset` has an option to load from a folder of compressed pickle files, all we have to do is pass the pickle_folder location.\n",
    "\n",
    "â„¹ï¸ For more information on the `CustomSpektralDataset` please check the [Graphs FAQ Section D](graphs_faq.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unravel.utils import CustomSpektralDataset\n",
    "\n",
    "dataset = CustomSpektralDataset(pickle_folder=pickle_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare for Training\n",
    "\n",
    "Now that we have all the data converted into Graphs inside our `CustomSpektralDataset` object, we can prepare to train the GNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Split Dataset\n",
    "\n",
    "Our `dataset` object has two custom methods to help split the data into train, test and validation sets.\n",
    "Either use `dataset.split_test_train()` if we don't need a validation set, or `dataset.split_test_train_validation()` if we do also require a validation set.\n",
    "\n",
    "We can split our data 'by_graph_id' if we have provided Graph Ids in our `SoccerGraphConverterPolars` using the 'graph_id' or 'graph_ids' parameter.\n",
    "\n",
    "The 'split_train', 'split_test' and 'split_validation' parameters can either be ratios, percentages or relative size compared to total. \n",
    "\n",
    "We opt to create a test, train _and_ validation set to use in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: CustomSpektralDataset(n_graphs=804)\n",
      "Test: CustomSpektralDataset(n_graphs=480)\n",
      "Validation: CustomSpektralDataset(n_graphs=338)\n"
     ]
    }
   ],
   "source": [
    "train, test, val = dataset.split_test_train_validation(\n",
    "    split_train=4, split_test=1, split_validation=1, by_graph_id=True, random_seed=42\n",
    ")\n",
    "print(\"Train:\", train)\n",
    "print(\"Test:\", test)\n",
    "print(\"Validation:\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ—’ï¸ We can see that, because we are splitting by only 4 different graph_ids here (the 4 match_ids) the ratio's aren't perfectly 4 to 1 to 1. If you change the `graph_id=match_id` parameter in the `SoccerGraphConverter` to `graph_ids=dummy_graph_ids(dataset)` you'll see that it's easier to get close to the correct ratios, simply because we have a lot more graph_ids to split a cross. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 5  # Increase for actual training\n",
    "batch_size = 32\n",
    "channels = 128\n",
    "n_layers = 3  # Number of CrystalConv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Build GNN Model\n",
    "\n",
    "This GNN Model has the same architecture as described in [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn/tree/main)\n",
    "\n",
    "This exact model can also simply be loaded as:\n",
    "\n",
    "`from unravel.classifiers import CrystalGraphClassifier` as shown in [Quick Start Guide](0_quick_start_guide.ipynb)\n",
    "\n",
    "Below we show the exact same code to make it easier to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GlobalAvgPool, CrystalConv\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "class CrystalGraphClassifier(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int = 3,\n",
    "        channels: int = 128,\n",
    "        drop_out: float = 0.5,\n",
    "        n_out: int = 1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.channels = channels\n",
    "        self.drop_out = drop_out\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = [CrystalConv() for _ in range(1, self.n_layers)]\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(self.channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(self.drop_out)\n",
    "        self.dense2 = Dense(self.channels, activation=\"relu\")\n",
    "        self.dense3 = Dense(self.n_out, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Create DataLoaders\n",
    "\n",
    "Create a Spektral [`DisjointLoader`](https://graphneural.network/loaders/#disjointloader). This DisjointLoader will help us to load batches of Disjoint Graphs for training purposes.\n",
    "\n",
    "Note that these Spektral `Loaders` return a generator, so if we want to retrain the model, we need to reload these loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import DisjointLoader\n",
    "\n",
    "loader_tr = DisjointLoader(train, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(val, epochs=1, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training and Prediction\n",
    "\n",
    "Below we outline how to train the model, make predictions and add the predicted values back to the Kloppy dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Compile Model\n",
    "\n",
    "1. Initialize the `CrystalGraphClassifier` (or create your own Graph Classifier).\n",
    "2. Compile the model with a loss function, optimizer and your preferred metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = CrystalGraphClassifier()\n",
    "\n",
    "model.compile(\n",
    "    loss=BinaryCrossentropy(), optimizer=Adam(), metrics=[AUC(), BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Fit Model\n",
    "\n",
    "1. We have a a [`DisjointLoader`](https://graphneural.network/loaders/#disjointloader) for training and validation sets.\n",
    "2. Fit the model. \n",
    "3. We add `EarlyStopping` and a `validation_data` dataset to monitor performance, and set `use_multiprocessing=True` to improve training speed.\n",
    "\n",
    "âš ï¸ When trying to fit the model _again_ make sure to reload Data Loaders in [Section 6.4](#64-create-dataloaders), because they are generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbekkers/PycharmProjects/unravelsports/.venv311/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 9ms/step - loss: 23.1703 - auc: 0.4863 - binary_accuracy: 0.4938 - val_loss: 2.4204 - val_auc: 0.5651 - val_binary_accuracy: 0.4822\n",
      "Epoch 2/5\n",
      "21/26 [=======================>......] - ETA: 0s - loss: 3.6232 - auc: 0.4877 - binary_accuracy: 0.4911 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 3ms/step - loss: 3.4254 - auc: 0.4817 - binary_accuracy: 0.4876\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.5735 - auc: 0.4856 - binary_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.1570 - auc: 0.5123 - binary_accuracy: 0.5137\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.0584 - auc: 0.5191 - binary_accuracy: 0.5112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x35bfcb010>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    epochs=5,\n",
    "    use_multiprocessing=True,\n",
    "    validation_data=loader_va.load(),\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Save & Load Model\n",
    "\n",
    "This step is solely included to show how to restore a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/my-first-graph-classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/my-first-graph-classifier/assets\n",
      "/Users/jbekkers/PycharmProjects/unravelsports/.venv311/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = \"models/my-first-graph-classifier\"\n",
    "model.save(model_path)\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Evaluate Model\n",
    "\n",
    "1. Create another `DisjointLoader`, this time for the test set.\n",
    "2. Evaluate model performance on the test set. This evaluation function uses the `metrics` passed to `model.compile`\n",
    "\n",
    "ğŸ—’ï¸ Our performance is really bad because we're using random labels, very few epochs and a small dataset.\n",
    "\n",
    "ğŸ“– For more information on evaluation in sports analytics see: [Methodology and evaluation in sports analytics: challenges, approaches, and lessons learned {J. Davis et. al. (2024)}](https://link.springer.com/article/10.1007/s10994-024-06585-0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7747 - auc: 0.4983 - binary_accuracy: 0.4729\n"
     ]
    }
   ],
   "source": [
    "loader_te = DisjointLoader(test, epochs=1, shuffle=False, batch_size=batch_size)\n",
    "results = model.evaluate(loader_te.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Predict on New Data\n",
    "\n",
    "1. Load new, unseen data from the SkillCorner dataset.\n",
    "2. Convert this data, making sure we use the exact same settings as in step 1.\n",
    "3. If we set `prediction=True` we do not have to supply labels to the `SoccerGraphConverterPolars`.\n",
    "4. We do still need to add graph_ids. It is advised to do this by \"frame_id\" for the prediction, such that we can more easily merge the predictions back to the correct frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kloppy_dataset = skillcorner.load_open_data(\n",
    "    match_id=2068,  # A game we have not yet used in section 4\n",
    "    include_empty_frames=False,\n",
    "    limit=500,\n",
    ")\n",
    "dataset = KloppyPolarsDataset(\n",
    "    kloppy_dataset=kloppy_dataset, ball_carrier_threshold=25.0\n",
    ")\n",
    "dataset.add_graph_ids(by=[\"frame_id\"])\n",
    "\n",
    "preds_converter = SoccerGraphConverterPolars(\n",
    "    dataset=dataset,\n",
    "    # Settings\n",
    "    prediction=True,\n",
    "    max_player_speed=12.0,\n",
    "    max_ball_speed=28.0,\n",
    "    self_loop_ball=True,\n",
    "    adjacency_matrix_connect_type=\"ball\",\n",
    "    adjacency_matrix_type=\"split_by_team\",\n",
    "    label_type=\"binary\",\n",
    "    defending_team_node_value=0.1,\n",
    "    non_potential_receiver_node_value=0.1,\n",
    "    random_seed=False,\n",
    "    pad=False,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a prediction on all the frames of this dataset using `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compute the graphs and add them to the CustomSpektralDataset\n",
    "pred_dataset = CustomSpektralDataset(graphs=preds_converter.to_spektral_graphs())\n",
    "\n",
    "loader_pred = DisjointLoader(\n",
    "    pred_dataset, batch_size=batch_size, epochs=1, shuffle=False\n",
    ")\n",
    "preds = model.predict(loader_pred.load(), use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a predictions Polars DataFrame\n",
    "\n",
    "We merge the `preds_df` to the `KloppyPolarsDataset` named `dataset` that we just applied the predictions to.\n",
    "\n",
    "ğŸ—’ï¸ We use `\"frame_id\"` here because the prediction dataset \"x.id\" is the Graph Ids we added above. We did this on the `\"frame_id\"` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>frame_id</th><th>period_id</th><th>timestamp</th><th>y_hat</th></tr><tr><td>i64</td><td>i64</td><td>duration[Î¼s]</td><td>f32</td></tr></thead><tbody><tr><td>2208</td><td>1</td><td>37s 500ms</td><td>0.449797</td></tr><tr><td>2209</td><td>1</td><td>37s 600ms</td><td>0.457843</td></tr><tr><td>2210</td><td>1</td><td>37s 700ms</td><td>0.428516</td></tr><tr><td>2211</td><td>1</td><td>37s 800ms</td><td>0.604198</td></tr><tr><td>2212</td><td>1</td><td>37s 900ms</td><td>0.491383</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ frame_id â”† period_id â”† timestamp    â”† y_hat    â”‚\n",
       "â”‚ ---      â”† ---       â”† ---          â”† ---      â”‚\n",
       "â”‚ i64      â”† i64       â”† duration[Î¼s] â”† f32      â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2208     â”† 1         â”† 37s 500ms    â”† 0.449797 â”‚\n",
       "â”‚ 2209     â”† 1         â”† 37s 600ms    â”† 0.457843 â”‚\n",
       "â”‚ 2210     â”† 1         â”† 37s 700ms    â”† 0.428516 â”‚\n",
       "â”‚ 2211     â”† 1         â”† 37s 800ms    â”† 0.604198 â”‚\n",
       "â”‚ 2212     â”† 1         â”† 37s 900ms    â”† 0.491383 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "preds_df = pl.DataFrame(\n",
    "    {\"frame_id\": [int(x.id) for x in pred_dataset], \"y_hat\": preds.flatten()}\n",
    ")\n",
    "preds_df.sort(\"y_hat\")\n",
    "\n",
    "dataset.data = dataset.data.join(preds_df, on=\"frame_id\", how=\"left\")\n",
    "\n",
    "dataset.data[300:305][[\"frame_id\", \"period_id\", \"timestamp\", \"y_hat\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
